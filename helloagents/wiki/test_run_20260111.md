# 2026-01-11 全量回归测试结果（单机 / Windows）

本报告记录“按仓库现有脚本可复现”的一次全量回归结果，便于对比与复跑；不代表生产容量上限。

## 环境

- OS: Windows（PowerShell）
- 后端：Spring Boot + Netty WS
- 依赖：本机 MySQL / Redis 可访问（`127.0.0.1:3306/6379`）
- JDK：`C:\Users\yejunbo\.jdks\openjdk-23.0.2\bin`

本次原始输出目录（含 mvn 日志/压测 JSON/k6 输出）：
- `logs/test-run-20260111_204501/`

> 注：脚本运行时使用了 `IM_MYSQL_PASSWORD` 环境变量；报告与日志中不包含明文密码。

---

## 1) Maven 单测与打包

命令：
- `mvn test`
- `mvn -DskipTests package`

结果：
- ✅ `mvn test exit=0`（存在 Mockito 动态 agent 警告，见 `logs/test-run-20260111_204501/mvn_test.err.log`）
- ✅ `mvn package exit=0`

---

## 2) WS 冒烟（单实例）

脚本：
- `scripts/ws-smoke-test/run.ps1`

结果文件：
- `logs/test-run-20260111_204501/ws_smoke_20260111_204937.json`

结论：
- ✅ `basic/idempotency/auth/friend_request` 通过
- ⚠️ `offline_drop_and_recover/cron_resend_no_ack_receive` 失败（原因：默认 `im.cron.resend.enabled=false`，导致兜底补发链路不触发；属于“配置预期”，但会被该用例判定为失败）
- ⚠️ Redis route key 检查被跳过：本机无 `redis-cli`（脚本已记录 `redis.skipped=true`，不影响主流程验证）

补齐验证（开启 cron 补发后重跑离线/定时用例）：
- `logs/test-run-20260111_204501/ws_smoke_offline_cronEnabled_20260111_214230.json`：`offline_drop_and_recover.ok=true`
- `logs/test-run-20260111_204501/ws_smoke_cron_cronEnabled_20260111_214230.json`：`cron_resend_no_ack_receive.ok=true`

---

## 3) 多实例冒烟（跨实例路由 + 单端登录 + 群聊）

脚本（两实例 + 冒烟 + moments + 负载一体）：
- `scripts/ws-backpressure-multi-test/run.ps1 -RunMomentsSmoke ...`

结果文件：
- `logs/test-run-20260111_204501/bp_multi_with_moments_20260111_212024.json`（wsA/wsB/httpA/httpB、KICK、跨实例收包）

结论：
- ✅ 跨实例单聊/群聊可达
- ✅ single-login KICK 可达（`ERROR reason=kicked`）

---

## 4) Moments 冒烟（两实例 HTTP）

脚本：
- `scripts/moments-smoke-test/run.ps1`

结果文件：
- `logs/bp-multi/moments_smoke_20260111_212044.json`

结论：
- ✅ 发起好友申请并互相成为好友
- ✅ 发布动态、好友时间线可见
- ✅ 点赞/取消点赞、评论/删除评论、删除动态：通过

---

## 5) 负载/背压/抖动场景

### 5.1 单实例基线（Java 压测器）

脚本：
- `scripts/ws-load-test/run.ps1`

结果文件：
- 连接压测：`logs/test-run-20260111_204501/ws_load_connect_20260111_212614.json`
  - ✅ 2000 连接：connect ok=2000/2000，auth ok=2000/2000
- RTT 基线：`logs/test-run-20260111_204501/ws_load_ping_20260111_212614.json`
  - PING/PONG：15000/15000；RTT：P50=16ms / P95=38ms / P99=47ms
- 单聊 E2E：`logs/test-run-20260111_204501/ws_load_single_e2e_20260111_212614.json`
  - 吞吐：sentPerSec≈976.67 msg/s
  - E2E：P50≈5.04s / P95≈10.14s / P99≈13.86s
- 连接抖动（flap + reconnect）：`logs/test-run-20260111_204501/ws_load_connect_flap_20260111_212614.json`
  - ✅ connect ok=6764（fail=0），auth ok=6764

### 5.2 多实例 + 慢端/不读（背压演练）

结果文件（本轮）：
- `logs/bp-multi/load_single_e2e_base_20260111_212044.json`
- `logs/bp-multi/load_single_e2e_slow_20260111_212044.json`
- `logs/bp-multi/load_single_e2e_noread_20260111_212044.json`
- `logs/bp-multi/mem_gw-a_20260111_212044.csv`
- `logs/bp-multi/mem_gw-b_20260111_212044.csv`

关键观察（参数：`BodyBytes=4000` + `BpLow/High=32/64KB` + `closeAfterMs=1500`）：
- ✅ 触发并执行“踢慢连接”闭环：`logs/bp-multi/gw-b_20260111_212030.out.log` 中 `ws backpressure: closing slow consumer channel` 计数 closeSlow=61
- 内存采样（WorkingSet）：
  - gw-a：min≈304.8MB / max≈667.39MB（30s 窗口）
  - gw-b：min≈306.78MB / max≈535.4MB（30s 窗口）
- ⚠️ 在该配置下 E2E 分位数为“十几秒到二十几秒级”（属于高写入 + 大包 + 小水位的强压组合，更多反映 DB/落库链路排队能力上限；不应把该 P99 直接归因于背压）

---

## 6) k6 回归（WS Ping / 单聊 E2E）

说明：k6 未预装，本次通过 GitHub release 临时下载 `k6.exe`（路径记录在 `logs/test-run-20260111_204501/_k6_path.txt`）。

### 6.1 WS Ping（k6）

---

## 附录 A) 2026-01-13：open-loop（固定速率）5实例对照回归（单聊）

目的：把压测从 closed-loop（ACK 越快发得越猛）改为 open-loop（固定节奏“尝试发送”），用 offered load（attempted）做公平对照，避免“优化后 ACK 更快反而把系统推到更高负载导致指标变差”的错觉。

脚本：
- `scripts/ws-cluster-5x-test/run.ps1`

关键参数：
- `-Instances 5 -DurationSmallSeconds 60 -MsgIntervalMs 3000 -OpenLoop -MaxInflightHard 200 -UserBase 29000000`
- E2E 口径保护：`-MaxValidE2eMs 600000`（默认 10min；用于过滤异常值，计入 e2eInvalid）

对照 1（ACK 合并开启，默认）：
- 结果目录：`logs/ws-cluster-5x-test_20260113_171143/`
- `single_e2e_5000.json`：
  - offered load：attemptedPerSec≈765.27 msg/s（sentPerSec≈765.27 msg/s，skippedHard=0）
  - E2E：P50≈1632ms / P95≈4326ms / P99≈4942ms
  - errors.wsError=310（<5%）
- `ws_perf_summary_gw1.json`（type=single_chat）：
  - queueMs：P50≈480ms / P95≈1246ms / P99≈1634ms
  - dbQueueMs：P50≈720ms / P95≈2583ms / P99≈2984ms
  - totalMs：P50≈1240ms / P95≈4100ms / P99≈4421ms

对照 2（ACK 合并关闭）：
- 结果目录：`logs/ws-cluster-5x-test_20260113_172430/`
- `single_e2e_5000.json`：
  - offered load：attemptedPerSec≈771.62 msg/s（sentPerSec≈771.62 msg/s，skippedHard=0）
  - E2E：P50≈1885ms / P95≈3415ms / P99≈3757ms
  - errors.wsError=135（<5%）
- `ws_perf_summary_gw1.json`（type=single_chat）：
  - queueMs：P50≈363ms / P95≈969ms / P99≈1241ms
  - dbQueueMs：P50≈841ms / P95≈1979ms / P99≈2255ms
  - totalMs：P50≈1373ms / P95≈3293ms / P99≈3694ms

初步解读（基于当前两轮 60s 样本）：
- 这组参数下的 E2E 主要由 `single_chat.dbQueueMs + single_chat.queueMs` 主导（ms~秒级），属于“DB/业务线程池排队 + 串行队列排队”的组合，而不是 Netty 写出背压本身。
- delivered/read ACK 合并（1s 窗口）对单聊消息 E2E 不在直接关键路径；两轮对照的 P95/P99 差异更可能来自系统抖动/噪声（需在更高 ACK 压力场景下单独验证该开关的收益/副作用）。

---

## 附录 B) delivered/read ACK 压力专测（窗口敏感性 + 多轮均值）

目的：在“delivered/read ACK 高密度”场景下，量化 `im.gateway.ws.ack.batch-enabled/batch-window-ms` 的收益与副作用，并用多轮均值降低噪声。

关键注意（否则会污染统计）：
- 压测/回归建议关闭 AUTH 后离线补发：`--im.gateway.ws.resend.after-auth-enabled=false`（否则历史消息会导致 `dup/reorder/e2eInvalid` 上升，P99 被污染）

压测工具：
- `scripts/ws-load-test/WsLoadTest.java` 新增 `ack_stress` 模式（接收端对每条 `SINGLE_CHAT` 发送 `ACK(delivered/read)`）
- `scripts/ws-cluster-5x-test/run.ps1` 支持 `-RunAckStress` + `-Repeats`，输出 `*_r1/r2/r3` 与 `*_avg.json`（均值为“每轮 P50/P95/P99 的平均”，不是把所有样本合并后的精确分位数）

统一参数（30s * 3 轮）：
- `-Instances 5 -DurationSmallSeconds 30 -DurationConnectSeconds 20 -WarmupMs 3000`
- 单聊 open-loop 对齐：`-MsgIntervalMs 3000 -OpenLoop -MaxInflightHard 200`
- ACK 压测（5k clients，delivered+read）：`-RunAckStress -AckStressMsgIntervalMs 500 -AckStressMaxInflightHard 2000 -AckStressTypes delivered,read -AckEveryN 1`

对照结果（均值文件路径）：
- 开启合并（1s）：`logs/ws-cluster-5x-test_20260113_202751/ack_stress_5000_avg.json`
  - acksSentPerSecAvg≈67.18/s（受消息投递速率限制）
  - wsErrorAvg≈2971
- 开启合并（200ms）：`logs/ws-cluster-5x-test_20260113_214922/ack_stress_5000_avg.json`
  - acksSentPerSecAvg≈221.31/s
  - wsErrorAvg≈6703
- 关闭合并：`logs/ws-cluster-5x-test_20260113_220525/ack_stress_5000_avg.json`
  - acksSentPerSecAvg≈323.44/s
  - wsErrorAvg≈9780

解读（当前实现下能得出的结论）：
- ACK 合并窗口越大，单位时间内真正发出的 delivered/read ACK 越少（这是预期行为：合并抑制“推进写放大”）。
- 在这组负载里，ACK 压测的上限被“消息投递/落库能力”限制；如果要把 ACK 压力进一步拉满，需要先把“被投递的消息数”拉上来（或设计不依赖消息投递的 ACK storm 形态）。
- 是否“收益为正”，主要看你希望优先压尾延迟/稳定性（降低 DB 写放大与拒绝概率），还是追求 delivered/read 的更细粒度实时性（更小窗口/不合并）。

输出：
- `logs/test-run-20260111_204501/k6_ws_ping_20260111_213420.out.log`

摘要（200 VUs，1m）：
- ws_auth_ok=200
- ws_pong_rtt_ms：avg≈8.9ms，p95≈11ms

### 6.2 单聊 E2E（k6）

输出（有效结果，使用全新 `UserBase` 避免离线补发历史消息污染）：
- `logs/test-run-20260111_204501/k6_ws_single_e2e_userbase20m_20260111_213635.out.log`

摘要（200 VUs，1m，MsgInterval=100ms）：
- im_sent_total≈61949
- im_recv_unique_total≈38869（dup=1）
- im_e2e_latency_ms：avg≈8.91s，p95≈21.44s

备注：若 `UserBase` 复用老用户，AUTH 时会触发离线补发历史消息，导致 e2e 指标被“旧消息的 sendTs”污染（本次已用新 userbase 规避）。

---

## 7) 总体结论（本次实测口径）

- 高并发连接：✅ 单实例 2000 连接/鉴权通过；抖动场景无连接失败（需更大规模与长稳态再评估上限）
- 即时性：⚠️ 本机 RTT P99≈47ms（Java ping），但在“写入/落库压力”下 E2E 可到 10s+（主要为排队延迟）
- 高可用：✅ 客户端侧断连重连（flap+reconnect）可恢复；⚠️ Redis/DB 侧故障仍需单独演练与量化（本轮未做停服级注入）
- 可靠性：✅ `ACK(saved)+ACK_RECEIVED` 链路正常；⚠️ 离线/兜底补发用例依赖配置 `im.cron.resend.enabled=true` 才会通过
- 有序性：⚠️ 压测中出现少量 reorder（例如 base 场景 reorder=12），属于“接收侧观测到的乱序”；发送侧单连接顺序依赖 `WsChannelSerialQueue` 与业务链路排队情况

---

# 2026-01-12 5 实例分级压测结果（单机 / Windows）

本段为“5 实例 + 分级压测 + 分段耗时归因”的一次可复现记录，主要用于定位瓶颈与对比回归；**不代表生产容量上限**。

本次原始输出目录：
- `logs/ws-cluster-5x-test_20260112_001222/`

实例（动态端口；同机共用 MySQL/Redis）：
- gw-1：http `59423` / ws `59428`
- gw-2：http `64769` / ws `64782`
- gw-3：http `63336` / ws `64049`
- gw-4：http `64107` / ws `64119`
- gw-5：http `64141` / ws `64393`

## 1) 多实例冒烟（2 实例）

结果文件：
- `logs/ws-cluster-5x-test_20260112_001222/smoke_cluster_2x.json`

结论：
- ✅ 跨实例路由可用（SINGLE_CHAT / GROUP_CHAT）
- ✅ 单端登录 KICK 可达（旧连接收到 `ERROR reason=kicked`）

## 2) 分级压测（500 / 5000 / 50000 连接）

### 2.1 CONNECT（仅连接+鉴权）

- 500：`logs/ws-cluster-5x-test_20260112_001222/connect_500.json`
  - ✅ connect ok=500/500，auth ok=500/500
- 5000：`logs/ws-cluster-5x-test_20260112_001222/connect_5000.json`
  - ✅ connect ok=5000/5000，auth ok=5000/5000
- 50000：`logs/ws-cluster-5x-test_20260112_001222/connect_50000.json`
  - ⚠️ connect ok=15909/50000（fail=34091）
  - 说明：此处更像是“单机压测端 + 本机端口/线程/FD 上限”的瓶颈，并不能直接推导服务端容量；如要评估 5w+ 连接需分布式压测端（多机/多进程）

### 2.2 PING（即时性基线）

- 500：`logs/ws-cluster-5x-test_20260112_001222/ping_500.json`
  - RTT：P50=12ms / P95=28ms / P99=36ms
- 5000：`logs/ws-cluster-5x-test_20260112_001222/ping_5000.json`
  - RTT：P50=78ms / P95=190ms / P99=228ms

### 2.3 单聊 E2E（inflight=4）

口径：发送端写入 `sendTs`（body 内），接收端收到 `SINGLE_CHAT` 时计算 `now-sendTs`。

- 500：`logs/ws-cluster-5x-test_20260112_001222/single_e2e_500.json`
  - 吞吐：sentPerSec≈1268.48 msg/s
  - E2E：P50≈861ms / P95≈1451ms / P99≈1682ms
  - 乱序/重复：dup=0，reorderByFrom=0
- 5000：`logs/ws-cluster-5x-test_20260112_001222/single_e2e_5000.json`
  - 吞吐：sentPerSec≈1716.68 msg/s；ackSaved=92996/103001（存在一定写入失败/拥塞）
  - E2E：P50≈9.70s / P95≈14.17s / P99≈15.22s
  - 乱序/重复：dup=0，reorderByFrom=0

## 3) 群聊 push E2E（200 人群 / 20 发送者）

结果文件：
- `logs/ws-cluster-5x-test_20260112_001222/group_push_e2e.json`

参数摘要：clients=200，senders=20，duration=60s，msgInterval=50ms，groupStrategyMode=push（强制推消息体）。

关键结果：
- 发送：sent=24231（≈403.85 msg/s），ackSaved=23862
- E2E：P50≈61ms / P95≈3003ms / P99≈3245ms
- 重复：dup=9750（sample receivers 口径）
- 乱序：
  - `reorderByFrom=9893`：同发送者维度的乱序/重复触发（多为“重复导致 seq<=last”，可用 client 去重降低）
  - `reorderByServerMsgId=839015`：跨发送者/跨实例 push 并发下的“全局 serverMsgId 非单调”（push 模式通常不保证强全序；需要客户端排序或改用 notify+pull）

## 4) “秒级~十几秒级 E2E”根因量化（ws_perf 分段）

解析口径：服务端开启 `im.gateway.ws.perf-trace.enabled=true` 后输出 `ws_perf ...`；脚本解析分位数：
- `logs/ws-cluster-5x-test_20260112_001222/ws_perf_summary_gw1.json`

结论（高影响环节优先）：
- SINGLE_CHAT：主要耗时不是 DB 查询/写入本身，而是排队
  - serial queue 排队（`queueMs`）：P95≈9538ms / P99≈10428ms
  - dbExecutor 排队（`dbQueueMs`）：P95≈2981ms / P99≈2996ms
  - db→eventLoop 回切排队（`dbToEventLoopMs`）：P95≈1882ms / P99≈2506ms
  - DB 写入段（`saveMsgMs/updateChatMs`）：P95≈10~11ms（相对较小）
- GROUP_CHAT：P95 total≈2961ms，其中 `queueMs`≈2914ms 为主，DB 写入段相对较小
- Redis/跨实例：`group_dispatch.routeMs/fanoutMs` 与 `redis_pubsub.totalMs` 均为几十毫秒以内，不是主要瓶颈

---

# 2026-01-12 5 实例对照实验：dbExecutor 调参 + 入站队列门禁（单机 / Windows）

目的：把 “E2E 秒级~十几秒级” 的排队瓶颈拆开，并用**调参（不大改代码）**验证“dbExecutor 排队/串行队列排队”的可控性与取舍。

对照两轮原始输出目录：
- 基线（默认参数）：`logs/ws-cluster-5x-test_20260112_162958/`
- 调参（db+入站门禁）：`logs/ws-cluster-5x-test_20260112_164609/`

## 1) 调参项

基线：使用默认配置（dbExecutor `core=8/max=32/queue=10000`；入站队列门禁关闭）。

调参：通过脚本启动参数注入（5 实例均生效）：
- `im.executors.db.core-pool-size=32`
- `im.executors.db.max-pool-size=64`
- `im.executors.db.queue-capacity=0`（倾向减少 dbQueue 排队，代价是更容易触发拒绝/回压）
- `im.gateway.ws.inbound-queue.enabled=true`
- `im.gateway.ws.inbound-queue.max-pending-per-conn=500`（倾向限制单连接的串行队列排队长度，代价是更多 `ERROR reason=server_busy`）

## 2) 单聊 E2E（5000 clients, 60s, inflight=4）

基线：`logs/ws-cluster-5x-test_20260112_162958/single_e2e_5000.json`
- 发送：sent=45972（sentPerSec≈766.20 msg/s）

---

# 2026-01-13 单聊尾延迟治理尝试：WS 编码 offload / post-db 隔离（单机 / Windows）

说明：
- 本轮目标是验证“把 eventLoop CPU 活（JSON 序列化）迁出 + 隔离落库后后置逻辑”是否能降低 `ws_perf single_chat.queueMs/dbToEventLoopMs` 并压低尾延迟。
- 实测过程中发现：如果直接把回调/编码引入新的线程池排队，会导致明显 E2E 回归（秒级~十几秒级），因此最终将 encode offload 默认关闭（需要显式开启）。

## 1) 代码变更点（仅记录现状，详见方案包）
- 方案包：`helloagents/plan/202601131700_ws_single_encode_postdb_isolation/`
- 新增 executors 配置：
  - `im.executors.ws-encode.*`（默认存在但功能默认关闭）
  - `im.executors.post-db.*`（预留）
- 新增开关：`im.gateway.ws.encode.enabled`（默认 false）

## 2) 5 实例分级压测（默认 encode 关闭）

脚本：
- `scripts/ws-cluster-5x-test/run.ps1`

本轮原始输出目录：
- `logs/ws-cluster-5x-test_20260113_114950/`

关键结果（5000 clients 单聊 E2E，inflight=4，60s）：
- `logs/ws-cluster-5x-test_20260113_114950/single_e2e_5000.json`
  - connect ok=5000/5000，auth ok=5000/5000
  - E2E：P50=1399ms / P95=10191ms / P99=13271ms
  - 乱序/重复：dup=0，reorderByFrom=0
  - wsError=8754（高压下存在明显写入失败/拥塞，需结合 `ws_perf` 与错误 reason 做进一步归因）

## 3) 多实例冒烟/背压演练

脚本：
- `scripts/ws-backpressure-multi-test/run.ps1`

结果文件：
- `logs/bp-multi/meta_20260113_120352.json`

结论：
- 跨实例路由、单端登录踢人、群聊链路：通过（见输出 steps 中的 `KICK` / `SINGLE_CHAT` / `GROUP_CHAT`）

- ACK(saved)：ackSaved=35980（ackSavedRate≈0.7827）
- E2E：P50≈10104ms / P95≈12621ms / P99≈14944ms
- 乱序/重复：dup=0，reorderByFrom=0，reorderByServerMsgId=0
- 错误：wsError=6852（主要为服务端 `ERROR` 回包统计）

调参：`logs/ws-cluster-5x-test_20260112_164609/single_e2e_5000.json`
- 发送：sent=75847（sentPerSec≈1264.12 msg/s）
- ACK(saved)：ackSaved=65853（ackSavedRate≈0.8682）
- E2E：P50≈3379ms / P95≈8799ms / P99≈11406ms
- 乱序/重复：dup=0，reorderByFrom=0，reorderByServerMsgId=0
- 错误：wsError=10105（入站门禁开启后，`server_busy` 错误占比上升，属于“用错误换尾延迟收敛”的取舍）

## 3) ws_perf 分段对照（gw-1）

基线：`logs/ws-cluster-5x-test_20260112_162958/ws_perf_summary_gw1.json`
- single_chat：totalMs P95≈11953ms；queueMs P95≈8980ms；dbQueueMs P95≈2986ms；dbToEventLoopMs P95≈765ms；DB 写入段 P95≈10ms 级

调参：`logs/ws-cluster-5x-test_20260112_164609/ws_perf_summary_gw1.json`
- single_chat：totalMs P95≈7069ms；queueMs P95≈6072ms；dbQueueMs P95≈0ms；dbToEventLoopMs P95≈2418ms；saveMsgMs P95≈38ms

解释：调参把 `dbQueueMs` 从秒级压到 0ms（基本消除 dbExecutor 排队），但把部分压力转移为“db 完成后回到 eventLoop 的排队”（`dbToEventLoopMs` 上升）。整体 E2E 仍显著改善，但必须接受：更高线程/更强限流意味着更多失败/回压事件（需要客户端退避重试策略配合）。

## 4) 连接上限说明（50000/500000）

- 50000 CONNECT（基线）：`logs/ws-cluster-5x-test_20260112_162958/connect_50000.json`，connect ok=16274/50000（fail=33726）
- 50000 CONNECT（调参）：`logs/ws-cluster-5x-test_20260112_164609/connect_50000.json`，connect ok=16207/50000（fail=33793）

结论：50k 连接在本机上主要卡在“压测端资源/本机网络栈/进程限制”，与服务端调参几乎无关；因此 **500000 连接无法在单机压测端得到有效结论**，需要多进程/多机或专用压测端（这是客观约束，不建议强行跑到 OOM）。

---

# 2026-01-12 慢消费者与背压演练复跑（内存/尾延迟）

本轮原始输出（含进程 PID/端口/日志路径）：
- `logs/bp-multi/meta_20260112_173529.json`

关键结果文件：
- base：`logs/bp-multi/load_single_e2e_base_20260112_173529.json`
- slow：`logs/bp-multi/load_single_e2e_slow_20260112_173529.json`
- noRead：`logs/bp-multi/load_single_e2e_noread_20260112_173529.json`
- 内存采样：`logs/bp-multi/mem_gw-a_20260112_173529.csv`、`logs/bp-multi/mem_gw-b_20260112_173529.csv`
- 慢端踢人日志：`logs/bp-multi/gw-b_20260112_173519.out.log`

参数摘要：clients=200，duration=60s，msgIntervalMs=10，bodyBytes=4000；slowConsumerPct=30，slowConsumerDelayMs=5000；noReadPct=30；写缓冲水位 32/64KB；closeUnwritableAfterMs=1500。

观测结果（本机 WorkingSet）：
- gw-a：min≈332.45MB / max≈684.32MB（65 个采样点）
- gw-b：min≈358.29MB / max≈583.75MB（65 个采样点）
- 慢端踢人：`ws backpressure: closing slow consumer channel`（gw-b 计数=56）

延迟与错误（注意：该组合为“强压”配置，wsError 大多来自 backpressure / server_busy）：
- slow：E2E P50≈8207ms / P99≈50921ms（slow 分位：P99≈56044ms）
- noRead：E2E P50≈10960ms / P99≈18737ms

结论：在“部分客户端不读/延迟读 + 高生产速率 + 大包”场景下，系统会出现明显的尾延迟放大；当前已具备“写缓冲水位 + 不可写踢人 + 写前 fail-fast”闭环，但仍会以 `ERROR`/丢弃/踢人为代价换取整体稳定（需要进一步把“哪些消息允许丢、哪些必须可靠补发”工程化区分）。

---

## 备注：脚本稳定性修复（避免残留 Java 进程）

发现：Windows 下 `java` 默认可能指向 `Oracle\\Java\\javapath\\java.exe`（shim），会导致 `Start-Process` 追踪到 shim PID 而不是实际 JVM PID，进而出现：
- 压测结束后残留大量 `java.exe -jar ...` 进程
- `mvn package` repackage 时 jar 被占用无法重命名
- 内存采样抓到 shim WorkingSet（≈9MB）而不是实际 JVM

修复：相关脚本已优先使用 `JAVA_HOME\\bin\\java.exe` 作为真实 JVM 路径，并在 `meta_*.json` 中记录 javaExe 与 PID，确保可复现与可观测。

---

# 2026-01-12 单聊 ACK 回切隔离（eventLoop 减负）回归

## 1) 改造点摘要（代码侧）

目标：减少“DB 完成后回到 eventLoop 的排队”，降低 `ws_perf single_chat.dbToEventLoopMs` 与 E2E 长尾。

- `WsWriter`：新增 `writeAck(Channel, ...)`；新增可选的 eventLoop 排队延迟回调（用于量化回切排队，不改口径作弊）。
- `WsSingleChatHandler`：去掉 `saveFuture.whenComplete -> ctx.executor().execute(...)` 的大闭包；在 DB 回调线程直接触发 ACK/push，写回由 `WsWriter` 自行 marshal 到目标 channel eventLoop。
- `WsGroupChatHandler`：**未按同样方式拆出 dispatch**（实测会放大群聊写洪峰并反噬 eventLoop，需额外并发/速率门控后再做）。

## 2) 5 实例对照（同参数、同 userBase=20000000）

调参前（仅参数调优，未改本轮代码）：
- `logs/ws-cluster-5x-test_20260112_164609/single_e2e_5000.json`
- `logs/ws-cluster-5x-test_20260112_164609/ws_perf_summary_gw1.json`

改造后（本轮 ACK 回切隔离）：
- `logs/ws-cluster-5x-test_20260112_200047/single_e2e_5000.json`
- `logs/ws-cluster-5x-test_20260112_200047/ws_perf_summary_gw1.json`

关键结论（单聊 5000）：

- E2E（客户端统计）
  - 调参前：P50≈3379ms / P95≈8799ms / P99≈11406ms；sentPerSec≈1264.12；ackSavedRate≈0.8682
  - 改造后：P50≈476ms / P95≈5271ms / P99≈7286ms；sentPerSec≈780.37；ackSavedRate≈0.7864

- `ws_perf single_chat`（gw-1 分段统计）
  - 调参前：totalMs P95≈7069ms；queueMs P95≈6072ms；dbToEventLoopMs P95≈2418ms
  - 改造后：totalMs P95≈4995ms；queueMs P95≈4145ms；dbToEventLoopMs P95≈1739ms

解释：单聊链路的主要收益来自 eventLoop 减负与回切排队收敛（`queueMs/dbToEventLoopMs` 同步下降）。吞吐与 ackSavedRate 有波动，可能受同机压测端 CPU 竞争与运行时抖动影响，建议对关键配置做 2-3 次重复跑取中位数。

## 3) E2E 指标“历史消息污染”说明与规避

当复用同一批 userId（同一个 `UserBase`）反复跑压测时，AUTH/补发/离线机制可能会向客户端推送历史消息，导致：

- 客户端侧 E2E P99 被“历史消息的 sendTs”污染（出现非常大的离群值）
- ACK/错误计数被“压测窗口外到达的回包”影响（比例失真）

规避策略：压测时为每轮回归选择新的 `UserBase`（确保是全新用户，无历史消息）。

本轮已复跑一轮“干净 userBase”以校验 E2E 分布（注意：该路径包含首次建会话/建群等冷启动成本）：

- `logs/ws-cluster-5x-test_20260112_203654/single_e2e_5000.json`（`-UserBase 30000000`）
  - E2E：P50≈214ms / P95≈7913ms / P99≈10652ms
- `logs/ws-cluster-5x-test_20260112_203654/group_push_e2e.json`（`-UserBase 30000000`）
  - 群聊 E2E：P50≈65ms / P95≈7508ms / P99≈9963ms

## 4) 慢消费者与背压演练：新增一轮复跑（20260112_205026）

本轮原始输出：
- `logs/bp-multi/meta_20260112_205026.json`

关键结果文件：
- base：`logs/bp-multi/load_single_e2e_base_20260112_205026.json`
- slow：`logs/bp-multi/load_single_e2e_slow_20260112_205026.json`
- noRead：`logs/bp-multi/load_single_e2e_noread_20260112_205026.json`
- 内存采样：`logs/bp-multi/mem_gw-a_20260112_205026.csv`、`logs/bp-multi/mem_gw-b_20260112_205026.csv`
- 慢端踢人日志：`logs/bp-multi/gw-b_20260112_205016.out.log`

观测摘要：
- gw-a WorkingSet：min≈311.99MB / max≈684.38MB（65 点）
- gw-b WorkingSet：min≈308.35MB / max≈551.86MB（65 点）
- 慢端踢人：`ws backpressure: closing slow consumer channel`（gw-b 计数=64）
- E2E（强压场景）：slow P99≈50226ms；base/noRead P99≈18570ms/14378ms（错误主要来自 backpressure/server_busy）

---

## 5) 补充：WS 编码 offload（实验开关）对照（2026-01-13）

目的：验证 `im.gateway.ws.encode.enabled=true`（JSON 编码从 eventLoop 迁出）在当前实现与参数下是否正收益。

压测入口：
- `powershell -ExecutionPolicy Bypass -File scripts/ws-cluster-5x-test/run.ps1 -Instances 5 -DurationSmallSeconds 60 -MsgIntervalMs 100 -Inflight 4`
- 开启对照：追加 `-WsEncodeEnabled`

结果摘要（5实例 / clients=5000 / 60s / MsgIntervalMs=100 / Inflight=4）：
- encode 关闭：`logs/ws-cluster-5x-test_20260113_152406/single_e2e_5000.json`，E2E `p50≈2997ms p95≈14846ms p99≈16649ms`
- encode 开启：`logs/ws-cluster-5x-test_20260113_150909/single_e2e_5000.json`，E2E `p50≈11258ms p95≈15907ms p99≈16101ms`

结论：在当前实现与当前机器环境下，该开关对单聊尾延迟收益为负，建议默认保持关闭，后续需要补齐 encode 队列排队（encodeQueueMs）观测与参数校准后再重试。
